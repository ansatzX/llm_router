# LLM Router Configuration

# Base URL of the LLM backend (e.g., SGLang, vLLM, or any OpenAI-compatible API)
LLM_BASE_URL=http://localhost:8000

# API Key for the LLM backend (if required, leave empty if not needed)
LLM_API_KEY=

# Port for the Flask router (default: 5001)
FLASK_PORT=5001

# ============================================================================
# Context Limit Configuration
# ============================================================================

# Maximum tokens for model output (default: 16384)
# Set to empty to disable cap and use caller's max_tokens
MAX_TOKENS_CAP=16384

# Max tools definition size in characters. If exceeded, lazy loading is enabled.
# Default: 40000 (~10k tokens)
MAX_TOOLS_CHARS=40000

# Max rounds for search_tools internal loop (default: 20)
MAX_TOOL_SEARCH_ROUNDS=20

# Max message content chars. Default 800000 (~200k tokens for Claude).
# For smaller context models, reduce this value:
#   - 21k token model: MAX_CONTEXT_CHARS=52000
#   - 32k token model: MAX_CONTEXT_CHARS=100000
#   - 128k token model: MAX_CONTEXT_CHARS=400000
MAX_CONTEXT_CHARS=800000

# ============================================================================
# Model Capability Configuration
# ============================================================================

# Model type: "text" or "multimodal"
# - text: Text-only model. Images/documents will be rejected with instructions
#         to use local tools (like MCP).
# - multimodal: Can process images directly. Documents will still be rejected
#               with instructions to use local tools.
MODEL_TYPE=text

# Maximum file size for uploads (in MB). Files larger than this will be rejected.
MAX_UPLOAD_SIZE_MB=10

# When a text model receives an image/document, return this message:
TEXT_MODEL_MEDIA_PROMPT=This model cannot process images or documents. Please use a local tool (e.g., MCP tool) to analyze the content and pass the results as text.

# When a multimodal model receives a document (not image), return this message:
MULTIMODAL_MODEL_DOCUMENT_PROMPT=This model cannot process documents directly. Please use a local tool (e.g., MCP tool) to extract the content and pass it as text.
